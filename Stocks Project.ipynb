{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning) # get rid of depreciation warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/jacobbills/Desktop/Economics/Twitter_sentiment_DJIA30/Combined_stocks.csv\") \n",
    "#brings in the csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some percents for the different sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14182 entries, 0 to 14181\n",
      "Data columns (total 56 columns):\n",
      "Unnamed: 0    14182 non-null int64\n",
      "Date          14182 non-null object\n",
      "HIGH          14182 non-null float64\n",
      "CLOSE         14182 non-null float64\n",
      "OPEN          14182 non-null float64\n",
      "LOW           14182 non-null float64\n",
      "NUM_NEG       14182 non-null int64\n",
      "NUM_NEU       14182 non-null int64\n",
      "NUM_POS       14182 non-null int64\n",
      "TW            14182 non-null int64\n",
      "weekend       14182 non-null int64\n",
      "NUM_NEG1      14182 non-null int64\n",
      "NUM_NEG2      14182 non-null int64\n",
      "NUM_NEG3      14182 non-null int64\n",
      "NUM_NEU1      14182 non-null int64\n",
      "NUM_NEU2      14182 non-null int64\n",
      "NUM_NEU3      14182 non-null int64\n",
      "NUM_POS1      14182 non-null int64\n",
      "NUM_POS2      14182 non-null int64\n",
      "NUM_POS3      14182 non-null int64\n",
      "TW1           14182 non-null int64\n",
      "TW2           14182 non-null int64\n",
      "TW3           14182 non-null int64\n",
      "CLOSE1        14182 non-null float64\n",
      "CLOSE2        14182 non-null float64\n",
      "CLOSE3        14182 non-null float64\n",
      "AXP           14182 non-null int64\n",
      "BA            14182 non-null int64\n",
      "CAT           14182 non-null int64\n",
      "CSCO          14182 non-null int64\n",
      "CVX           14182 non-null int64\n",
      "DD            14182 non-null int64\n",
      "DIS           14182 non-null int64\n",
      "GE            14182 non-null int64\n",
      "GS            14182 non-null int64\n",
      "HD            14182 non-null int64\n",
      "IBM           14182 non-null int64\n",
      "INTC          14182 non-null int64\n",
      "JNJ           14182 non-null int64\n",
      "JPM           14182 non-null int64\n",
      "KO            14182 non-null int64\n",
      "MCD           14182 non-null int64\n",
      "MMM           14182 non-null int64\n",
      "MRK           14182 non-null int64\n",
      "MSFT          14182 non-null int64\n",
      "NKE           14182 non-null int64\n",
      "PFE           14182 non-null int64\n",
      "PG            14182 non-null int64\n",
      "T             14182 non-null int64\n",
      "TRV           14182 non-null int64\n",
      "UNH           14182 non-null int64\n",
      "UTX           14182 non-null int64\n",
      "V             14182 non-null int64\n",
      "VZ            14182 non-null int64\n",
      "WMT           14182 non-null int64\n",
      "XOM           14182 non-null int64\n",
      "dtypes: float64(7), int64(48), object(1)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info() #checking what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create industry type variables\n",
    "def industrifier(df,list_):\n",
    "    for i in range(len(df)):\n",
    "        for e in list_:\n",
    "            for x in e:\n",
    "                if df.loc[i,x] == 1:\n",
    "                    df.loc[i,str(e)] = 1\n",
    "                else:\n",
    "                    pass\n",
    "    for e in list_:\n",
    "        df.drop(columns=e, inplace=True)\n",
    "    df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create percentages\n",
    "sentiments = ['NUM_NEG', 'NUM_NEG1', 'NUM_NEG2', 'NUM_NEG3','NUM_NEU','NUM_NEU1','NUM_NEU2','NUM_NEU3','NUM_POS',\n",
    "              'NUM_POS1','NUM_POS2', 'NUM_POS3']\n",
    "for e in sentiments:\n",
    "    if e.endswith(\"1\"):\n",
    "        data[e+\"_PER\"] = data[e]/data['TW1']\n",
    "    elif e.endswith(\"2\"):\n",
    "        data[e+\"_PER\"] = data[e]/data['TW2']\n",
    "    elif e.endswith(\"3\"):\n",
    "        data[e+\"_PER\"] = data[e]/data['TW3']\n",
    "    else:\n",
    "        data[e+\"_PER\"] = data[e]/data['TW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummies for each day\n",
    "data['Date']= pd.to_datetime(data['Date'])\n",
    "data['day'] = data['Date'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>OPEN</th>\n",
       "      <th>LOW</th>\n",
       "      <th>NUM_NEG</th>\n",
       "      <th>NUM_NEU</th>\n",
       "      <th>NUM_POS</th>\n",
       "      <th>TW</th>\n",
       "      <th>...</th>\n",
       "      <th>NUM_POS1_PER</th>\n",
       "      <th>NUM_POS2_PER</th>\n",
       "      <th>NUM_POS3_PER</th>\n",
       "      <th>day_Friday</th>\n",
       "      <th>day_Monday</th>\n",
       "      <th>day_Saturday</th>\n",
       "      <th>day_Sunday</th>\n",
       "      <th>day_Thursday</th>\n",
       "      <th>day_Tuesday</th>\n",
       "      <th>day_Wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>77.24</td>\n",
       "      <td>76.02</td>\n",
       "      <td>76.50</td>\n",
       "      <td>75.96</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-06-05</td>\n",
       "      <td>76.28</td>\n",
       "      <td>74.79</td>\n",
       "      <td>76.04</td>\n",
       "      <td>74.64</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-06-06</td>\n",
       "      <td>76.25</td>\n",
       "      <td>76.24</td>\n",
       "      <td>74.75</td>\n",
       "      <td>74.64</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-06-07</td>\n",
       "      <td>78.12</td>\n",
       "      <td>78.03</td>\n",
       "      <td>76.68</td>\n",
       "      <td>76.45</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-06-08</td>\n",
       "      <td>78.12</td>\n",
       "      <td>78.03</td>\n",
       "      <td>76.68</td>\n",
       "      <td>76.45</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       Date   HIGH  CLOSE   OPEN    LOW  NUM_NEG  NUM_NEU  \\\n",
       "0           0 2013-06-04  77.24  76.02  76.50  75.96        2       12   \n",
       "1           1 2013-06-05  76.28  74.79  76.04  74.64        0        4   \n",
       "2           2 2013-06-06  76.25  76.24  74.75  74.64        1        9   \n",
       "3           3 2013-06-07  78.12  78.03  76.68  76.45        2        4   \n",
       "4           4 2013-06-08  78.12  78.03  76.68  76.45        0        3   \n",
       "\n",
       "   NUM_POS  TW      ...        NUM_POS1_PER  NUM_POS2_PER  NUM_POS3_PER  \\\n",
       "0        6  20      ...            0.090909      0.000000      0.000000   \n",
       "1        2   6      ...            0.300000      0.090909      0.000000   \n",
       "2        0  10      ...            0.333333      0.300000      0.090909   \n",
       "3       11  17      ...            0.000000      0.333333      0.300000   \n",
       "4        3   6      ...            0.647059      0.000000      0.333333   \n",
       "\n",
       "   day_Friday  day_Monday  day_Saturday  day_Sunday  day_Thursday  \\\n",
       "0           0           0             0           0             0   \n",
       "1           0           0             0           0             0   \n",
       "2           0           0             0           0             1   \n",
       "3           1           0             0           0             0   \n",
       "4           0           0             1           0             0   \n",
       "\n",
       "   day_Tuesday  day_Wednesday  \n",
       "0            1              0  \n",
       "1            0              1  \n",
       "2            0              0  \n",
       "3            0              0  \n",
       "4            0              0  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =pd.get_dummies(data, columns=['day'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NUM_NEG_PER', 'NUM_NEG1_PER', 'NUM_NEG2_PER', 'NUM_NEG3_PER',\n",
       "       'NUM_NEU_PER', 'NUM_NEU1_PER', 'NUM_NEU2_PER', 'NUM_NEU3_PER',\n",
       "       'NUM_POS_PER', 'NUM_POS1_PER', 'NUM_POS2_PER', 'NUM_POS3_PER'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_null = data.columns[data.isnull().any()] # seeing where there might be a problem\n",
    "is_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14654 entries, 0 to 14653\n",
      "Data columns (total 12 columns):\n",
      "NUM_NEG_PER     14620 non-null float64\n",
      "NUM_NEG1_PER    14620 non-null float64\n",
      "NUM_NEG2_PER    14619 non-null float64\n",
      "NUM_NEG3_PER    14619 non-null float64\n",
      "NUM_NEU_PER     14620 non-null float64\n",
      "NUM_NEU1_PER    14620 non-null float64\n",
      "NUM_NEU2_PER    14619 non-null float64\n",
      "NUM_NEU3_PER    14619 non-null float64\n",
      "NUM_POS_PER     14620 non-null float64\n",
      "NUM_POS1_PER    14620 non-null float64\n",
      "NUM_POS2_PER    14619 non-null float64\n",
      "NUM_POS3_PER    14619 non-null float64\n",
      "dtypes: float64(12)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "data[is_null].info() # about 30 missing for each, for whatever reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in is_null: #just fill them in with whatever is closest\n",
    "    data[e].fillna(data[e].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)): # create the classes\n",
    "    if data.loc[i,\"CLOSE\"] > data.loc[i,\"OPEN\"]+.25:\n",
    "        data.loc[i,\"class\"] = 1 # gained by more than $.25\n",
    "    elif data.loc[i,\"CLOSE\"] < data.loc[i,'OPEN']-.25:\n",
    "        data.loc[i,\"class\"] = 2 # lost by more than $.25 \n",
    "    else:\n",
    "        data.loc[i,\"class\"] = 0 # minimal change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>OPEN</th>\n",
       "      <th>Date</th>\n",
       "      <th>weekend</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.02</td>\n",
       "      <td>76.50</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74.79</td>\n",
       "      <td>76.04</td>\n",
       "      <td>2013-06-05</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76.24</td>\n",
       "      <td>74.75</td>\n",
       "      <td>2013-06-06</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78.03</td>\n",
       "      <td>76.68</td>\n",
       "      <td>2013-06-07</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.03</td>\n",
       "      <td>76.68</td>\n",
       "      <td>2013-06-08</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>78.03</td>\n",
       "      <td>76.68</td>\n",
       "      <td>2013-06-09</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>78.27</td>\n",
       "      <td>77.98</td>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>76.52</td>\n",
       "      <td>77.61</td>\n",
       "      <td>2013-06-11</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>74.72</td>\n",
       "      <td>76.91</td>\n",
       "      <td>2013-06-12</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>75.21</td>\n",
       "      <td>74.46</td>\n",
       "      <td>2013-06-13</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>72.96</td>\n",
       "      <td>74.78</td>\n",
       "      <td>2013-06-14</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>72.96</td>\n",
       "      <td>74.78</td>\n",
       "      <td>2013-06-15</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>72.96</td>\n",
       "      <td>74.78</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>73.86</td>\n",
       "      <td>73.69</td>\n",
       "      <td>2013-06-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>74.99</td>\n",
       "      <td>74.10</td>\n",
       "      <td>2013-06-18</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>74.24</td>\n",
       "      <td>74.87</td>\n",
       "      <td>2013-06-19</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>72.95</td>\n",
       "      <td>73.76</td>\n",
       "      <td>2013-06-20</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>73.33</td>\n",
       "      <td>73.38</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>73.33</td>\n",
       "      <td>73.38</td>\n",
       "      <td>2013-06-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>73.33</td>\n",
       "      <td>73.38</td>\n",
       "      <td>2013-06-23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>72.03</td>\n",
       "      <td>72.57</td>\n",
       "      <td>2013-06-24</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>73.23</td>\n",
       "      <td>72.63</td>\n",
       "      <td>2013-06-25</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>73.91</td>\n",
       "      <td>73.64</td>\n",
       "      <td>2013-06-26</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>75.14</td>\n",
       "      <td>74.46</td>\n",
       "      <td>2013-06-27</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>74.69</td>\n",
       "      <td>75.00</td>\n",
       "      <td>2013-06-28</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>74.69</td>\n",
       "      <td>75.00</td>\n",
       "      <td>2013-06-29</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>74.69</td>\n",
       "      <td>75.00</td>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>75.64</td>\n",
       "      <td>75.53</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>74.62</td>\n",
       "      <td>75.43</td>\n",
       "      <td>2013-07-02</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>74.37</td>\n",
       "      <td>74.02</td>\n",
       "      <td>2013-07-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14624</th>\n",
       "      <td>99.71</td>\n",
       "      <td>99.55</td>\n",
       "      <td>2014-08-20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14625</th>\n",
       "      <td>99.29</td>\n",
       "      <td>99.80</td>\n",
       "      <td>2014-08-21</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14626</th>\n",
       "      <td>98.50</td>\n",
       "      <td>98.90</td>\n",
       "      <td>2014-08-22</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14627</th>\n",
       "      <td>98.50</td>\n",
       "      <td>98.90</td>\n",
       "      <td>2014-08-23</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14628</th>\n",
       "      <td>98.50</td>\n",
       "      <td>98.90</td>\n",
       "      <td>2014-08-24</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14629</th>\n",
       "      <td>98.78</td>\n",
       "      <td>98.58</td>\n",
       "      <td>2014-08-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14630</th>\n",
       "      <td>99.65</td>\n",
       "      <td>99.12</td>\n",
       "      <td>2014-08-26</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14631</th>\n",
       "      <td>99.49</td>\n",
       "      <td>99.78</td>\n",
       "      <td>2014-08-27</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14632</th>\n",
       "      <td>99.61</td>\n",
       "      <td>99.04</td>\n",
       "      <td>2014-08-28</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14633</th>\n",
       "      <td>99.45</td>\n",
       "      <td>99.38</td>\n",
       "      <td>2014-08-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14634</th>\n",
       "      <td>99.45</td>\n",
       "      <td>99.38</td>\n",
       "      <td>2014-08-30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>99.45</td>\n",
       "      <td>99.38</td>\n",
       "      <td>2014-08-31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>99.45</td>\n",
       "      <td>99.38</td>\n",
       "      <td>2014-09-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>98.46</td>\n",
       "      <td>99.43</td>\n",
       "      <td>2014-09-02</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>99.11</td>\n",
       "      <td>98.86</td>\n",
       "      <td>2014-09-03</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>98.36</td>\n",
       "      <td>99.00</td>\n",
       "      <td>2014-09-04</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14640</th>\n",
       "      <td>99.19</td>\n",
       "      <td>98.74</td>\n",
       "      <td>2014-09-05</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14641</th>\n",
       "      <td>99.19</td>\n",
       "      <td>98.74</td>\n",
       "      <td>2014-09-06</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14642</th>\n",
       "      <td>99.19</td>\n",
       "      <td>98.74</td>\n",
       "      <td>2014-09-07</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14643</th>\n",
       "      <td>97.74</td>\n",
       "      <td>98.92</td>\n",
       "      <td>2014-09-08</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14644</th>\n",
       "      <td>97.39</td>\n",
       "      <td>97.80</td>\n",
       "      <td>2014-09-09</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14645</th>\n",
       "      <td>96.82</td>\n",
       "      <td>97.38</td>\n",
       "      <td>2014-09-10</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14646</th>\n",
       "      <td>97.01</td>\n",
       "      <td>96.32</td>\n",
       "      <td>2014-09-11</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14647</th>\n",
       "      <td>95.77</td>\n",
       "      <td>96.53</td>\n",
       "      <td>2014-09-12</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14648</th>\n",
       "      <td>95.77</td>\n",
       "      <td>96.53</td>\n",
       "      <td>2014-09-13</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14649</th>\n",
       "      <td>95.77</td>\n",
       "      <td>96.53</td>\n",
       "      <td>2014-09-14</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14650</th>\n",
       "      <td>96.28</td>\n",
       "      <td>95.73</td>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14651</th>\n",
       "      <td>97.43</td>\n",
       "      <td>96.23</td>\n",
       "      <td>2014-09-16</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14652</th>\n",
       "      <td>97.08</td>\n",
       "      <td>97.83</td>\n",
       "      <td>2014-09-17</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14653</th>\n",
       "      <td>96.62</td>\n",
       "      <td>97.15</td>\n",
       "      <td>2014-09-18</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14654 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CLOSE   OPEN       Date  weekend  class\n",
       "0      76.02  76.50 2013-06-04        0    2.0\n",
       "1      74.79  76.04 2013-06-05        0    2.0\n",
       "2      76.24  74.75 2013-06-06        0    1.0\n",
       "3      78.03  76.68 2013-06-07        0    1.0\n",
       "4      78.03  76.68 2013-06-08        1    1.0\n",
       "5      78.03  76.68 2013-06-09        1    1.0\n",
       "6      78.27  77.98 2013-06-10        0    1.0\n",
       "7      76.52  77.61 2013-06-11        0    2.0\n",
       "8      74.72  76.91 2013-06-12        0    2.0\n",
       "9      75.21  74.46 2013-06-13        0    1.0\n",
       "10     72.96  74.78 2013-06-14        0    2.0\n",
       "11     72.96  74.78 2013-06-15        1    2.0\n",
       "12     72.96  74.78 2013-06-16        1    2.0\n",
       "13     73.86  73.69 2013-06-17        0    0.0\n",
       "14     74.99  74.10 2013-06-18        0    1.0\n",
       "15     74.24  74.87 2013-06-19        0    2.0\n",
       "16     72.95  73.76 2013-06-20        0    2.0\n",
       "17     73.33  73.38 2013-06-21        0    0.0\n",
       "18     73.33  73.38 2013-06-22        1    0.0\n",
       "19     73.33  73.38 2013-06-23        1    0.0\n",
       "20     72.03  72.57 2013-06-24        0    2.0\n",
       "21     73.23  72.63 2013-06-25        0    1.0\n",
       "22     73.91  73.64 2013-06-26        0    1.0\n",
       "23     75.14  74.46 2013-06-27        0    1.0\n",
       "24     74.69  75.00 2013-06-28        0    2.0\n",
       "25     74.69  75.00 2013-06-29        1    2.0\n",
       "26     74.69  75.00 2013-06-30        1    2.0\n",
       "27     75.64  75.53 2013-07-01        0    0.0\n",
       "28     74.62  75.43 2013-07-02        0    2.0\n",
       "29     74.37  74.02 2013-07-03        0    1.0\n",
       "...      ...    ...        ...      ...    ...\n",
       "14624  99.71  99.55 2014-08-20        0    0.0\n",
       "14625  99.29  99.80 2014-08-21        0    2.0\n",
       "14626  98.50  98.90 2014-08-22        0    2.0\n",
       "14627  98.50  98.90 2014-08-23        1    2.0\n",
       "14628  98.50  98.90 2014-08-24        1    2.0\n",
       "14629  98.78  98.58 2014-08-25        0    0.0\n",
       "14630  99.65  99.12 2014-08-26        0    1.0\n",
       "14631  99.49  99.78 2014-08-27        0    2.0\n",
       "14632  99.61  99.04 2014-08-28        0    1.0\n",
       "14633  99.45  99.38 2014-08-29        0    0.0\n",
       "14634  99.45  99.38 2014-08-30        1    0.0\n",
       "14635  99.45  99.38 2014-08-31        1    0.0\n",
       "14636  99.45  99.38 2014-09-01        1    0.0\n",
       "14637  98.46  99.43 2014-09-02        0    2.0\n",
       "14638  99.11  98.86 2014-09-03        0    0.0\n",
       "14639  98.36  99.00 2014-09-04        0    2.0\n",
       "14640  99.19  98.74 2014-09-05        0    1.0\n",
       "14641  99.19  98.74 2014-09-06        1    1.0\n",
       "14642  99.19  98.74 2014-09-07        1    1.0\n",
       "14643  97.74  98.92 2014-09-08        0    2.0\n",
       "14644  97.39  97.80 2014-09-09        0    2.0\n",
       "14645  96.82  97.38 2014-09-10        0    2.0\n",
       "14646  97.01  96.32 2014-09-11        0    1.0\n",
       "14647  95.77  96.53 2014-09-12        0    2.0\n",
       "14648  95.77  96.53 2014-09-13        1    2.0\n",
       "14649  95.77  96.53 2014-09-14        1    2.0\n",
       "14650  96.28  95.73 2014-09-15        0    1.0\n",
       "14651  97.43  96.23 2014-09-16        0    1.0\n",
       "14652  97.08  97.83 2014-09-17        0    2.0\n",
       "14653  96.62  97.15 2014-09-18        0    2.0\n",
       "\n",
       "[14654 rows x 5 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[[\"CLOSE\", \"OPEN\", \"Date\", \"weekend\",\"class\"]] #see how the data looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>OPEN</th>\n",
       "      <th>LOW</th>\n",
       "      <th>NUM_NEG</th>\n",
       "      <th>NUM_NEU</th>\n",
       "      <th>NUM_POS</th>\n",
       "      <th>TW</th>\n",
       "      <th>...</th>\n",
       "      <th>NUM_POS2_PER</th>\n",
       "      <th>NUM_POS3_PER</th>\n",
       "      <th>day_Friday</th>\n",
       "      <th>day_Monday</th>\n",
       "      <th>day_Saturday</th>\n",
       "      <th>day_Sunday</th>\n",
       "      <th>day_Thursday</th>\n",
       "      <th>day_Tuesday</th>\n",
       "      <th>day_Wednesday</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>77.24</td>\n",
       "      <td>76.02</td>\n",
       "      <td>76.50</td>\n",
       "      <td>75.96</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-06-05</td>\n",
       "      <td>76.28</td>\n",
       "      <td>74.79</td>\n",
       "      <td>76.04</td>\n",
       "      <td>74.64</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-06-06</td>\n",
       "      <td>76.25</td>\n",
       "      <td>76.24</td>\n",
       "      <td>74.75</td>\n",
       "      <td>74.64</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-06-07</td>\n",
       "      <td>78.12</td>\n",
       "      <td>78.03</td>\n",
       "      <td>76.68</td>\n",
       "      <td>76.45</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>78.61</td>\n",
       "      <td>78.27</td>\n",
       "      <td>77.98</td>\n",
       "      <td>77.69</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       Date   HIGH  CLOSE   OPEN    LOW  NUM_NEG  NUM_NEU  \\\n",
       "0           0 2013-06-04  77.24  76.02  76.50  75.96        2       12   \n",
       "1           1 2013-06-05  76.28  74.79  76.04  74.64        0        4   \n",
       "2           2 2013-06-06  76.25  76.24  74.75  74.64        1        9   \n",
       "3           3 2013-06-07  78.12  78.03  76.68  76.45        2        4   \n",
       "6           6 2013-06-10  78.61  78.27  77.98  77.69        0        6   \n",
       "\n",
       "   NUM_POS  TW  ...    NUM_POS2_PER  NUM_POS3_PER  day_Friday  day_Monday  \\\n",
       "0        6  20  ...        0.000000      0.000000           0           0   \n",
       "1        2   6  ...        0.090909      0.000000           0           0   \n",
       "2        0  10  ...        0.300000      0.090909           0           0   \n",
       "3       11  17  ...        0.333333      0.300000           1           0   \n",
       "6        7  13  ...        0.500000      0.647059           0           1   \n",
       "\n",
       "   day_Saturday  day_Sunday  day_Thursday  day_Tuesday  day_Wednesday  class  \n",
       "0             0           0             0            1              0    2.0  \n",
       "1             0           0             0            0              1    2.0  \n",
       "2             0           0             1            0              0    1.0  \n",
       "3             0           0             0            0              0    1.0  \n",
       "6             0           0             0            0              0    1.0  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.loc[data.weekend==0]\n",
    "data.head() #drop out the weekend data since it doesn't really tell us anything and see if it worked (it did)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data.CLOSE # target\n",
    "target_class = data[\"class\"] # target for classifiers\n",
    "high = data.HIGH # other columns that we don't really need for now\n",
    "low = data.LOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.drop(columns=['CLOSE', 'HIGH', 'LOW', 'Unnamed: 0', 'class', 'day_Saturday', 'day_Sunday']) # get rid of those ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nodate = data1.drop(columns='Date') # create a df without the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prev = data_nodate.drop(columns=['NUM_NEG', 'NUM_NEU', 'NUM_POS', 'NUM_NEG_PER', 'NUM_NEU_PER', 'NUM_POS_PER', \"TW\"])\n",
    "# create a df without anything for the day of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nopen_date = data_nodate.drop(columns='OPEN')\n",
    "data_nopen_prev = data_prev.drop(columns='OPEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn everything into floats so that scaler is happy\n",
    "\n",
    "data_nodate = pd.DataFrame(data_nodate, dtype='float')\n",
    "data_prev = pd.DataFrame(data_prev, dtype='float')\n",
    "data_nopen_date = pd.DataFrame(data_nopen_date, dtype='float')\n",
    "data_nopen_prev = pd.DataFrame(data_nopen_prev, dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10159 entries, 0 to 14653\n",
      "Data columns (total 60 columns):\n",
      "weekend          10159 non-null float64\n",
      "NUM_NEG1         10159 non-null float64\n",
      "NUM_NEG2         10159 non-null float64\n",
      "NUM_NEG3         10159 non-null float64\n",
      "NUM_NEU1         10159 non-null float64\n",
      "NUM_NEU2         10159 non-null float64\n",
      "NUM_NEU3         10159 non-null float64\n",
      "NUM_POS1         10159 non-null float64\n",
      "NUM_POS2         10159 non-null float64\n",
      "NUM_POS3         10159 non-null float64\n",
      "TW1              10159 non-null float64\n",
      "TW2              10159 non-null float64\n",
      "TW3              10159 non-null float64\n",
      "CLOSE1           10159 non-null float64\n",
      "CLOSE2           10159 non-null float64\n",
      "CLOSE3           10159 non-null float64\n",
      "AXP              10159 non-null float64\n",
      "BA               10159 non-null float64\n",
      "CAT              10159 non-null float64\n",
      "CSCO             10159 non-null float64\n",
      "CVX              10159 non-null float64\n",
      "DD               10159 non-null float64\n",
      "DIS              10159 non-null float64\n",
      "GE               10159 non-null float64\n",
      "GS               10159 non-null float64\n",
      "HD               10159 non-null float64\n",
      "IBM              10159 non-null float64\n",
      "INTC             10159 non-null float64\n",
      "JNJ              10159 non-null float64\n",
      "JPM              10159 non-null float64\n",
      "KO               10159 non-null float64\n",
      "MCD              10159 non-null float64\n",
      "MMM              10159 non-null float64\n",
      "MRK              10159 non-null float64\n",
      "MSFT             10159 non-null float64\n",
      "NKE              10159 non-null float64\n",
      "PFE              10159 non-null float64\n",
      "PG               10159 non-null float64\n",
      "T                10159 non-null float64\n",
      "TRV              10159 non-null float64\n",
      "UNH              10159 non-null float64\n",
      "UTX              10159 non-null float64\n",
      "V                10159 non-null float64\n",
      "VZ               10159 non-null float64\n",
      "WMT              10159 non-null float64\n",
      "XOM              10159 non-null float64\n",
      "NUM_NEG1_PER     10159 non-null float64\n",
      "NUM_NEG2_PER     10159 non-null float64\n",
      "NUM_NEG3_PER     10159 non-null float64\n",
      "NUM_NEU1_PER     10159 non-null float64\n",
      "NUM_NEU2_PER     10159 non-null float64\n",
      "NUM_NEU3_PER     10159 non-null float64\n",
      "NUM_POS1_PER     10159 non-null float64\n",
      "NUM_POS2_PER     10159 non-null float64\n",
      "NUM_POS3_PER     10159 non-null float64\n",
      "day_Friday       10159 non-null float64\n",
      "day_Monday       10159 non-null float64\n",
      "day_Thursday     10159 non-null float64\n",
      "day_Tuesday      10159 non-null float64\n",
      "day_Wednesday    10159 non-null float64\n",
      "dtypes: float64(60)\n",
      "memory usage: 4.7 MB\n"
     ]
    }
   ],
   "source": [
    "data_nopen_prev.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is clean, we can try to run some different models on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() #scale each of the three sets with open\n",
    "\n",
    "scaler.fit(data_nodate)\n",
    "scaled_df_noo = scaler.transform(data_nodate)\n",
    "\n",
    "scaler.fit(data_prev)\n",
    "scaled_prevo = scaler.transform(data_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break into test and training sets\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(scaled_df_noo, target, test_size=.2, random_state = 11)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(scaled_prevo, target, test_size=.2, random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_learning_curve(clf, \"Learning Curve\", X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.6948907948158737\n",
      "Test MSE: 0.6032102402205742\n"
     ]
    }
   ],
   "source": [
    "# elastic net without dates\n",
    "clf = ElasticNetCV(l1_ratio=[.1, .3, .5, .7, .8, .9, .95, .99, 1], alphas= [.1, 1, 5, 10], cv=5) #basic Elastic Net with CV\n",
    "clf.fit(X_train1, y_train1)\n",
    "train_predictions = clf.predict(X_train1)\n",
    "test_predictions = clf.predict(X_test1)\n",
    "print(\"Train MSE: {}\".format(mean_squared_error(y_train1, train_predictions)))\n",
    "print(\"Test MSE: {}\".format(mean_squared_error(y_test1, test_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NUM_NEG', -0.0),\n",
       " ('NUM_NEU', -0.0),\n",
       " ('NUM_POS', 0.0),\n",
       " ('TW', -0.0),\n",
       " ('weekend', 0.0),\n",
       " ('NUM_NEG1', -0.0),\n",
       " ('NUM_NEG2', 0.0),\n",
       " ('NUM_NEG3', 0.0),\n",
       " ('NUM_NEU1', -0.0),\n",
       " ('NUM_NEU2', -0.0),\n",
       " ('NUM_NEU3', -0.0),\n",
       " ('NUM_POS1', -0.0),\n",
       " ('NUM_POS2', 0.0),\n",
       " ('NUM_POS3', 0.0),\n",
       " ('TW1', -0.0),\n",
       " ('TW2', 0.0),\n",
       " ('TW3', 0.0),\n",
       " ('CLOSE2', 0.0),\n",
       " ('CLOSE3', 0.0),\n",
       " ('AXP', -0.0),\n",
       " ('BA', 0.0),\n",
       " ('CAT', 0.0),\n",
       " ('CSCO', -0.0),\n",
       " ('CVX', 0.0),\n",
       " ('DD', -0.0),\n",
       " ('DIS', -0.0),\n",
       " ('GE', -0.0),\n",
       " ('GS', 0.0),\n",
       " ('HD', -0.0),\n",
       " ('IBM', 0.0),\n",
       " ('INTC', -0.0),\n",
       " ('JNJ', 0.0),\n",
       " ('JPM', -0.0),\n",
       " ('KO', -0.0),\n",
       " ('MCD', 0.0),\n",
       " ('MMM', 0.0),\n",
       " ('MRK', -0.0),\n",
       " ('MSFT', -0.0),\n",
       " ('NKE', -0.0),\n",
       " ('PFE', -0.0),\n",
       " ('PG', -0.0),\n",
       " ('T', -0.0),\n",
       " ('TRV', 0.0),\n",
       " ('UNH', 0.0),\n",
       " ('UTX', 0.0),\n",
       " ('V', 0.0),\n",
       " ('VZ', -0.0),\n",
       " ('WMT', 0.0),\n",
       " ('XOM', 0.0),\n",
       " ('NUM_NEG1_PER', -0.0),\n",
       " ('NUM_NEG2_PER', 0.0),\n",
       " ('NUM_NEG3_PER', 0.0),\n",
       " ('NUM_NEU_PER', -0.0),\n",
       " ('NUM_NEU1_PER', 0.0),\n",
       " ('NUM_NEU2_PER', -0.0),\n",
       " ('NUM_NEU3_PER', -0.0),\n",
       " ('NUM_POS1_PER', -0.0),\n",
       " ('NUM_POS2_PER', 0.0),\n",
       " ('NUM_POS3_PER', 0.0),\n",
       " ('day_Friday', 0.0),\n",
       " ('day_Monday', -0.0),\n",
       " ('day_Thursday', -0.0),\n",
       " ('day_Tuesday', -0.0),\n",
       " ('day_Wednesday', 0.0),\n",
       " ('CLOSE1', 0.000500390133490317),\n",
       " ('NUM_POS_PER', 0.006114901424223976),\n",
       " ('NUM_NEG_PER', -0.017814296221278735),\n",
       " ('OPEN', 45.02708238077944)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as above\n",
    "sorted(list(zip(data_nodate, clf.coef_)), key=lambda x: abs(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.7003492633051123\n",
      "Test MSE: 0.6075214545452414\n"
     ]
    }
   ],
   "source": [
    "# elastic net without today's data\n",
    "clf = ElasticNetCV(l1_ratio=[.1, .3, .5, .7, .8, .9, .95, .99, 1], alphas= [.1, 1, 5, 10], cv=5) #basic Elastic Net with CV\n",
    "clf.fit(X_train2, y_train2)\n",
    "train_predictions = clf.predict(X_train2)\n",
    "test_predictions = clf.predict(X_test2)\n",
    "print(\"Train MSE: {}\".format(mean_squared_error(y_train2, train_predictions)))\n",
    "print(\"Test MSE: {}\".format(mean_squared_error(y_test2, test_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weekend', 0.0),\n",
       " ('NUM_NEG1', -0.0),\n",
       " ('NUM_NEG2', 0.0),\n",
       " ('NUM_NEG3', 0.0),\n",
       " ('NUM_NEU1', -0.0),\n",
       " ('NUM_NEU2', -0.0),\n",
       " ('NUM_NEU3', -0.0),\n",
       " ('NUM_POS1', -0.0),\n",
       " ('NUM_POS2', 0.0),\n",
       " ('NUM_POS3', 0.0),\n",
       " ('TW1', -0.0),\n",
       " ('TW2', 0.0),\n",
       " ('TW3', 0.0),\n",
       " ('CLOSE2', 0.0),\n",
       " ('CLOSE3', 0.0),\n",
       " ('AXP', -0.0),\n",
       " ('BA', 0.0),\n",
       " ('CAT', 0.0),\n",
       " ('CSCO', -0.0),\n",
       " ('CVX', 0.0),\n",
       " ('DD', -0.0),\n",
       " ('DIS', -0.0),\n",
       " ('GE', -0.0),\n",
       " ('GS', 0.0),\n",
       " ('HD', -0.0),\n",
       " ('IBM', 0.0),\n",
       " ('INTC', -0.0),\n",
       " ('JNJ', 0.0),\n",
       " ('JPM', -0.0),\n",
       " ('KO', -0.0),\n",
       " ('MCD', 0.0),\n",
       " ('MMM', 0.0),\n",
       " ('MRK', -0.0),\n",
       " ('MSFT', -0.0),\n",
       " ('NKE', -0.0),\n",
       " ('PFE', -0.0),\n",
       " ('PG', -0.0),\n",
       " ('T', -0.0),\n",
       " ('TRV', 0.0),\n",
       " ('UNH', 0.0),\n",
       " ('UTX', 0.0),\n",
       " ('V', 0.0),\n",
       " ('VZ', -0.0),\n",
       " ('WMT', 0.0),\n",
       " ('XOM', 0.0),\n",
       " ('NUM_NEG1_PER', -0.0),\n",
       " ('NUM_NEG2_PER', 0.0),\n",
       " ('NUM_NEG3_PER', 0.0),\n",
       " ('NUM_NEU1_PER', 0.0),\n",
       " ('NUM_NEU2_PER', -0.0),\n",
       " ('NUM_NEU3_PER', -0.0),\n",
       " ('NUM_POS1_PER', -0.0),\n",
       " ('NUM_POS2_PER', 0.0),\n",
       " ('NUM_POS3_PER', 0.0),\n",
       " ('day_Friday', 0.0),\n",
       " ('day_Monday', -0.0),\n",
       " ('day_Thursday', -0.0),\n",
       " ('day_Tuesday', -0.0),\n",
       " ('day_Wednesday', 0.0),\n",
       " ('CLOSE1', 0.0004528626066566547),\n",
       " ('OPEN', 45.02575353669932)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(zip(data_prev, clf.coef_)), key=lambda x: abs(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data wihtout open\n",
    "scaler.fit(data_nopen_date)\n",
    "scaled_df_non = scaler.transform(data_nopen_date)\n",
    "\n",
    "scaler.fit(data_nopen_prev)\n",
    "scaled_prevn = scaler.transform(data_nopen_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training sets\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(scaled_df_non, target, test_size=.2, random_state = 11)\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(scaled_prevn, target, test_size=.2, random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.9595450259558125\n",
      "Test MSE: 0.7892453635364851\n",
      "[('NUM_NEG', -0.0), ('NUM_NEU', -0.0), ('NUM_POS', 0.0), ('TW', -0.0), ('weekend', 0.0), ('NUM_NEG1', -0.0), ('NUM_NEG2', 0.0), ('NUM_NEG3', 0.0), ('NUM_NEU1', -0.0), ('NUM_NEU2', -0.0), ('NUM_NEU3', -0.0), ('NUM_POS1', -0.0), ('NUM_POS2', -0.0), ('NUM_POS3', 0.0), ('TW1', -0.0), ('TW2', -0.0), ('TW3', -0.0), ('AXP', -0.0), ('BA', 0.0), ('CAT', 0.0), ('CSCO', -0.0), ('CVX', 0.0), ('DD', -0.0), ('DIS', -0.0), ('GE', -0.0), ('GS', 0.0), ('HD', -0.0), ('IBM', 0.0), ('INTC', -0.0), ('JNJ', 0.0), ('JPM', 0.0), ('KO', -0.0), ('MCD', 0.0), ('MMM', 0.0), ('MRK', -0.0), ('MSFT', -0.0), ('NKE', -0.0), ('PFE', -0.0), ('PG', -0.0), ('T', -0.0), ('TRV', 0.0), ('UNH', -0.0), ('UTX', 0.0), ('V', 0.0), ('VZ', -0.0), ('WMT', 0.0), ('XOM', 0.0), ('NUM_NEG1_PER', -0.0), ('NUM_NEG2_PER', 0.0), ('NUM_NEG3_PER', 0.0), ('NUM_NEU_PER', -0.0), ('NUM_NEU1_PER', 0.0), ('NUM_NEU2_PER', -0.0), ('NUM_NEU3_PER', -0.0), ('NUM_POS1_PER', -0.0), ('NUM_POS2_PER', 0.0), ('NUM_POS3_PER', 0.0), ('day_Friday', 0.0), ('day_Monday', 0.0), ('day_Thursday', -0.0), ('day_Tuesday', 0.0), ('day_Wednesday', -0.0), ('CLOSE3', 0.0010741218345864207), ('CLOSE2', 0.001555947389059509), ('NUM_POS_PER', 0.05652381766938113), ('NUM_NEG_PER', -0.09928719796715095), ('CLOSE1', 45.01713465175223)]\n"
     ]
    }
   ],
   "source": [
    "# Elastic net with no dates\n",
    "clf = ElasticNetCV(l1_ratio=[.1, .3, .5, .7, .8, .9, .95, .99, 1], alphas= [.1, 1, 5, 10], cv=5) #basic Elastic Net with CV\n",
    "clf.fit(X_train4, y_train4)\n",
    "train_predictions = clf.predict(X_train4)\n",
    "test_predictions = clf.predict(X_test4)\n",
    "print(\"Train MSE: {}\".format(mean_squared_error(y_train4, train_predictions)))\n",
    "print(\"Test MSE: {}\".format(mean_squared_error(y_test4, test_predictions)))\n",
    "print(sorted(list(zip(data_nopen_date, clf.coef_)), key=lambda x: abs(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.0074518409183346\n",
      "Test MSE: 0.8312476433891135\n",
      "[('weekend', 0.0), ('NUM_NEG1', -0.0), ('NUM_NEG2', -0.0), ('NUM_NEG3', 0.0), ('NUM_NEU1', -0.0), ('NUM_NEU2', -0.0), ('NUM_NEU3', -0.0), ('NUM_POS1', -0.0), ('NUM_POS2', -0.0), ('NUM_POS3', 0.0), ('TW1', -0.0), ('TW2', -0.0), ('TW3', -0.0), ('CLOSE2', 0.0), ('AXP', -0.0), ('BA', 0.0), ('CAT', 0.0), ('CSCO', -0.0), ('CVX', 0.0), ('DD', -0.0), ('DIS', 0.0), ('GE', -0.0), ('GS', 0.0), ('HD', -0.0), ('IBM', 0.0), ('INTC', -0.0), ('JNJ', 0.0), ('JPM', -0.0), ('KO', -0.0), ('MCD', -0.0), ('MMM', 0.0), ('MRK', -0.0), ('MSFT', -0.0), ('NKE', 0.0), ('PFE', -0.0), ('PG', -0.0), ('T', -0.0), ('TRV', 0.0), ('UNH', 0.0), ('UTX', 0.0), ('V', 0.0), ('VZ', -0.0), ('WMT', -0.0), ('XOM', 0.0), ('NUM_NEG1_PER', -0.0), ('NUM_NEG2_PER', 0.0), ('NUM_NEG3_PER', 0.0), ('NUM_NEU1_PER', 0.0), ('NUM_NEU2_PER', -0.0), ('NUM_NEU3_PER', -0.0), ('NUM_POS1_PER', 0.0), ('NUM_POS2_PER', 0.0), ('NUM_POS3_PER', 0.0), ('day_Friday', 0.0), ('day_Monday', 0.0), ('day_Thursday', -0.0), ('day_Tuesday', 0.0), ('day_Wednesday', -0.0), ('CLOSE3', 0.00029561969707829423), ('CLOSE1', 45.01217715910246)]\n"
     ]
    }
   ],
   "source": [
    "# elastic net without anything from the day being predicted\n",
    "clf = ElasticNetCV(l1_ratio=[.1, .3, .5, .7, .8, .9, .95, .99, 1], alphas= [.1, 1, 5, 10], cv=5) #basic Elastic Net with CV\n",
    "clf.fit(X_train5, y_train5)\n",
    "train_predictions = clf.predict(X_train5)\n",
    "test_predictions = clf.predict(X_test5)\n",
    "print(\"Train MSE: {}\".format(mean_squared_error(y_train5, train_predictions)))\n",
    "print(\"Test MSE: {}\".format(mean_squared_error(y_test5, test_predictions)))\n",
    "print(sorted(list(zip(data_nopen_prev, clf.coef_)), key=lambda x: abs(x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, it isn't very interesting to use the data to predict future prices because it ends up that basically the only thing you want to know is what the opening price/yesterday's price is, and then how people were tweeting the day of (possibly after closing). So instead we'll try to predict if tweets can forecast if the stock is going to go up or down. The \"previous\" data is the most important sort here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier #Nearest Neighbors\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, GaussianNB # different Bayesian models\n",
    "from sklearn.svm import SVC #support vector machine\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier #ensemble models\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV #to do cross validation\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error #different ways of scoring\n",
    "from sklearn.metrics import classification_report # classification report\n",
    "from sklearn.metrics import confusion_matrix # confusion matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training sets from the previous day data (with and without OPEN)\n",
    "X_traincO, X_testcO, y_traincO, y_testcO = train_test_split(scaled_prevo, target_class, test_size=.2, random_state = 11)\n",
    "X_trainc, X_testc, y_trainc, y_testc = train_test_split(scaled_prevn, target_class, test_size=.2, random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'n_neighbors': 11, 'p': 3, 'weights': 'distance'}\n",
      "Train F1: [1. 1. 1.]\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.65      0.59       810\n",
      "         1.0       0.42      0.43      0.43       630\n",
      "         2.0       0.39      0.29      0.33       592\n",
      "\n",
      "   micro avg       0.47      0.47      0.47      2032\n",
      "   macro avg       0.45      0.45      0.45      2032\n",
      "weighted avg       0.46      0.47      0.46      2032\n",
      "\n",
      "Train Accuracy: 1.0\tTest accuracy: 0.4749015748031496\n"
     ]
    }
   ],
   "source": [
    "# K Nearest Neighbors with open\n",
    "knn = KNeighborsClassifier()\n",
    "gridsearch = GridSearchCV(knn, {\"n_neighbors\": [5, 7, 9, 11], \"weights\": ['uniform', 'distance'], \n",
    "                                'p': [1, 2, 3]}, scoring='f1_micro', cv=3)\n",
    "gridsearch.fit(X_traincO, y_traincO)\n",
    "print(\"Best Params: {}\".format(gridsearch.best_params_))\n",
    "y_pred_train = gridsearch.predict(X_traincO)\n",
    "print(\"Train F1: {}\".format(f1_score(y_traincO, y_pred_train, average=None)))\n",
    "print(\"Test Classification Report:\")\n",
    "y_pred_test = gridsearch.predict(X_testcO)\n",
    "print(classification_report(y_testcO, y_pred_test))\n",
    "print(\"Train Accuracy: {}\\tTest accuracy: {}\".format(accuracy_score(y_traincO, y_pred_train),\n",
    "                                                     accuracy_score(y_testcO, y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[525, 166, 119],\n",
       "       [215, 271, 144],\n",
       "       [222, 201, 169]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matric to evaluate performance\n",
    "confusion_matrix(y_testcO, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'n_neighbors': 11, 'p': 3, 'weights': 'distance'}\n",
      "Train F1: [1. 1. 1.]\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.65      0.59       810\n",
      "         1.0       0.43      0.43      0.43       630\n",
      "         2.0       0.39      0.29      0.33       592\n",
      "\n",
      "   micro avg       0.48      0.48      0.48      2032\n",
      "   macro avg       0.46      0.46      0.45      2032\n",
      "weighted avg       0.46      0.48      0.47      2032\n",
      "\n",
      "Train Accuracy: 1.0\tTest accuracy: 0.4763779527559055\n",
      "[[525 165 120]\n",
      " [212 272 146]\n",
      " [220 201 171]]\n"
     ]
    }
   ],
   "source": [
    "# K Nearest Neighbors without open\n",
    "knn = KNeighborsClassifier()\n",
    "gridsearch = GridSearchCV(knn, {\"n_neighbors\": [5, 7, 9, 11], \"weights\": ['uniform', 'distance'], \n",
    "                                'p': [1, 2, 3]}, scoring='f1_micro', cv=3)\n",
    "gridsearch.fit(X_trainc, y_trainc)\n",
    "print(\"Best Params: {}\".format(gridsearch.best_params_))\n",
    "y_pred_train = gridsearch.predict(X_trainc)\n",
    "print(\"Train F1: {}\".format(f1_score(y_trainc, y_pred_train, average=None)))\n",
    "print(\"Test Classification Report:\")\n",
    "y_pred_test = gridsearch.predict(X_testc)\n",
    "print(classification_report(y_testc, y_pred_test))\n",
    "print(\"Train Accuracy: {}\\tTest accuracy: {}\".format(accuracy_score(y_trainc, y_pred_train),\n",
    "                                                     accuracy_score(y_testc, y_pred_test)))\n",
    "print(confusion_matrix(y_testc, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first of all this model is way overfitting, so that needs to be fixed. Removing \"OPEN\" made it only marginally better (talking like 2 more true positives for each class) so that indicates that OPEN is not a driving force for this problem (unlike the basic prediction problem). Secondly, it seems to mostly put things in the first class (no major change) which works okay but not great. It is really bad at predicting a drop in prices, so I'm not sure what is going on there, except that it probaly trained itself to mostly put things in the first class. I'll try a random forest, since we can get feature importances from that which may give valuable information about what variables to keep and what ones to drop so that we stop overfitting the nearest neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'class_weight': 'balanced', 'max_depth': 11, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'n_estimators': 500, 'random_state': 9}\n",
      "Train F1: [0.66584982 0.76710796 0.73928158]\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.53      0.58       810\n",
      "         1.0       0.38      0.40      0.39       630\n",
      "         2.0       0.33      0.39      0.36       592\n",
      "\n",
      "   micro avg       0.45      0.45      0.45      2032\n",
      "   macro avg       0.45      0.44      0.44      2032\n",
      "weighted avg       0.47      0.45      0.46      2032\n",
      "\n",
      "Train Accuracy: 0.7226528854435831\tTest accuracy: 0.4498031496062992\n",
      "[[426 171 213]\n",
      " [121 255 254]\n",
      " [114 245 233]]\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier with Open\n",
    "rfc = RandomForestClassifier()\n",
    "gridsearch = GridSearchCV(rfc, {\"n_estimators\": [20, 50, 100, 200, 500], \"max_depth\": [1, 3, 5, 7, 9, 11], \n",
    "                                 \"min_samples_leaf\": [3 , 5 , 7 , 9], \"max_features\": [None, 'sqrt', 'log2']\n",
    "                                , \"class_weight\": [\"balanced\", \"balanced_subsample\"], 'random_state': [9]}, \n",
    "                          scoring='f1_micro', cv=3)\n",
    "gridsearch.fit(X_traincO, y_traincO)\n",
    "print(\"Best Params: {}\".format(gridsearch.best_params_))\n",
    "y_pred_train = gridsearch.predict(X_traincO)\n",
    "print(\"Train F1: {}\".format(f1_score(y_traincO, y_pred_train, average=None)))\n",
    "print(\"Test Classification Report:\")\n",
    "y_pred_test = gridsearch.predict(X_testcO)\n",
    "print(classification_report(y_testcO, y_pred_test))\n",
    "print(\"Train Accuracy: {}\\tTest accuracy: {}\".format(accuracy_score(y_traincO, y_pred_train),\n",
    "                                                     accuracy_score(y_testcO, y_pred_test)))\n",
    "print(confusion_matrix(y_testcO, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcb = RandomForestClassifier(n_estimators = 500, max_features='sqrt', min_samples_leaf=3, max_depth\n",
    "                             = 11, class_weight='balanced')\n",
    "rfcb.fit(X_traincO, y_traincO)\n",
    "feature_imp_rfc1 = sorted(list(zip(data_prev, rfcb.feature_importances_)), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'class_weight': 'balanced', 'max_depth': 11, 'max_features': None, 'min_samples_leaf': 3, 'n_estimators': 200, 'random_state': 9}\n",
      "Train F1: [0.7497006  0.82321598 0.80287156]\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.53      0.58       810\n",
      "         1.0       0.38      0.39      0.38       630\n",
      "         2.0       0.33      0.40      0.36       592\n",
      "\n",
      "   micro avg       0.45      0.45      0.45      2032\n",
      "   macro avg       0.45      0.44      0.44      2032\n",
      "weighted avg       0.47      0.45      0.46      2032\n",
      "\n",
      "Train Accuracy: 0.7903285345145811\tTest accuracy: 0.4478346456692913\n",
      "[[430 162 218]\n",
      " [126 246 258]\n",
      " [117 241 234]]\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier without Open\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "gridsearch = GridSearchCV(rfc, {\"n_estimators\": [20, 50, 100, 200, 500], \"max_depth\": [1, 3, 5, 7, 9, 11], \n",
    "                                 \"min_samples_leaf\": [3 , 5 , 7 , 9], \"max_features\": [None, 'sqrt', 'log2']\n",
    "                                , \"class_weight\": [\"balanced\", \"balanced_subsample\"], 'random_state': [9]}, \n",
    "                          scoring='f1_micro', cv=3)\n",
    "gridsearch.fit(X_trainc, y_trainc)\n",
    "print(\"Best Params: {}\".format(gridsearch.best_params_))\n",
    "y_pred_train = gridsearch.predict(X_trainc)\n",
    "print(\"Train F1: {}\".format(f1_score(y_trainc, y_pred_train, average=None)))\n",
    "print(\"Test Classification Report:\")\n",
    "y_pred_test = gridsearch.predict(X_testc)\n",
    "print(classification_report(y_testc, y_pred_test))\n",
    "print(\"Train Accuracy: {}\\tTest accuracy: {}\".format(accuracy_score(y_trainc, y_pred_train),\n",
    "                                                     accuracy_score(y_testc, y_pred_test)))\n",
    "print(confusion_matrix(y_testc, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcb2 = RandomForestClassifier(class_weight= 'balanced', max_depth= 11, max_features= None, min_samples_leaf= 3, \n",
    "                               n_estimators= 200, random_state= 9)\n",
    "rfcb2.fit(X_trainc, y_trainc)\n",
    "feature_imp_rfc2 = sorted(list(zip(data_nopen_prev, rfcb2.feature_importances_)), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll now use an old set of functions I wrote to get plot out the most important features and then decide how to engineer from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_extract(dic):\n",
    "    y = pd.DataFrame.from_dict(dic, orient='index') #turns the dictionary into a data frame\n",
    "    y = y.index # list of the names in the dataframe\n",
    "    return y\n",
    "\n",
    "def feature_extract(data_frame, feature_importances, sensitivity): # function to extract a list of features above a given sensitivty\n",
    "    coef_dict = dict((zip(data_frame, feature_importances))) #creates a dictionary from the model's feature importance\n",
    "    coef_list = {} # another dictionary\n",
    "    for key in coef_dict: # runs through the zipped dictionary \n",
    "        x = coef_dict.get(key) # extracts value for the key\n",
    "        if abs(x) > sensitivity:  # if the absolute value is bigger than the sensitivity\n",
    "            coef_list[key] = x # adds a new entry to the other dictionary \n",
    "        else: # if the coefficient isn't big enough\n",
    "            pass # move to the next key\n",
    "    coef_list = key_extract(coef_list) # uses a previous function I wrote to make a list of the names of those coeff\n",
    "    return coef_list # returns that list\n",
    "\n",
    "def best_feature_dict(list_, features_imp): #creates a dictionary from a list of tuples using another list of names\n",
    "    y = {}\n",
    "    for e in list_:\n",
    "        y[e] = dict(features_imp).get(e)\n",
    "    return y\n",
    "\n",
    "def best_feature_plot(list_, features_imp, ticks=90): #plots the features importances\n",
    "    y = best_feature_dict(list_, features_imp)\n",
    "    plt.bar(y.keys(),y.values())\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.xticks(rotation=ticks)\n",
    "    plt.ylabel(\"Percent Used\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "def features_full(data_frame, feature_importances, sensitivity): #combines these functions into one nice one\n",
    "    x = feature_extract(data_frame, feature_importances, sensitivity)\n",
    "    z = sorted(list(zip(data_frame, feature_importances)), key=lambda x: x[1], reverse=True)\n",
    "    y = best_feature_plot(x, z)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_feat1, best_featplt1 = features_full(data_prev, rfcb.feature_importances_, .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_feat2, best_featplt2 = features_full(data_nopen_prev,rfcb2.feature_importances_, .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the two models are quite similar and most importantly, sentiments does play a role in how these get classified. Next, we'll look at correlations between these higher importance features and then maybe mess with the data set a bit more to get a less overfitting k-nearest neighbors model (since that one is still working the best). Might try and SVM too, since lower amounts of features helps a lot with that. Also, once again, having OPEN doesn't really make a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_extract(var, sensitivity): # definine a function that will tell me where there is high correlation\n",
    "    y = dict() #the plan is to get a dictionary that gives me the name and value\n",
    "    count = 0 # set up the counter\n",
    "    for e in var:\n",
    "        if abs(e)<sensitivity: #most common is lower correlation so I'll get rid of those first\n",
    "            count = count + 1 #move to the next one\n",
    "        elif abs(e)==1: # get rid of perfect correlation because that isn't very interesting\n",
    "            count = count + 1 #\n",
    "        else: \n",
    "            x = var.index[count] #extract the name from the series\n",
    "            y[x] = e #key: index, value: correlation\n",
    "            count = count + 1 # move to the next one\n",
    "    return y \n",
    "\n",
    "def i_corr_extract(list_corr, var, sensitivity): #for doing a correlation extract except iterated\n",
    "    for x in list_corr:\n",
    "        print(x)\n",
    "        print(corr_extract(var[x], sensitivity))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_NEG1\n",
      "{'NUM_NEG2': 0.40170196998031343, 'NUM_NEU1': 0.45451223511486377, 'NUM_POS1': 0.3093336961341877, 'TW1': 0.5852331398670126, 'NUM_NEG1_PER': 0.45800710807257766}\n",
      "\n",
      "NUM_NEG2\n",
      "{'NUM_NEG1': 0.40170196998031343, 'NUM_NEG3': 0.38798199354234814, 'NUM_NEU2': 0.46218589722712966, 'NUM_POS2': 0.3084674758077302, 'TW2': 0.5793150972614369, 'NUM_NEG2_PER': 0.4202407650999104}\n",
      "\n",
      "NUM_NEG3\n",
      "{'NUM_NEG2': 0.38798199354234814, 'NUM_NEU3': 0.47631873900540306, 'NUM_POS3': 0.304764618599842, 'TW3': 0.6027214587212899, 'NUM_NEG3_PER': 0.38717757134382996}\n",
      "\n",
      "NUM_NEU1\n",
      "{'NUM_NEG1': 0.45451223511486377, 'NUM_NEU2': 0.5337631597425944, 'NUM_NEU3': 0.3246033459685328, 'NUM_POS1': 0.5075420215691235, 'NUM_POS2': 0.3484913425862472, 'TW1': 0.9603049811683173, 'TW2': 0.5385755397393015, 'TW3': 0.3055004638263273}\n",
      "\n",
      "NUM_NEU2\n",
      "{'NUM_NEG2': 0.46218589722712966, 'NUM_NEU1': 0.5337631597425944, 'NUM_NEU3': 0.501403598019858, 'NUM_POS2': 0.4764761594909855, 'TW1': 0.5144154500339283, 'TW2': 0.9613166884037355, 'TW3': 0.4816590399585326}\n",
      "\n",
      "NUM_NEU3\n",
      "{'NUM_NEG3': 0.47631873900540306, 'NUM_NEU1': 0.3246033459685328, 'NUM_NEU2': 0.501403598019858, 'NUM_POS3': 0.5127505131212996, 'TW1': 0.3062832897109994, 'TW2': 0.4817824410448253, 'TW3': 0.9580926218354631}\n",
      "\n",
      "NUM_POS1\n",
      "{'NUM_NEG1': 0.3093336961341877, 'NUM_NEU1': 0.5075420215691235, 'NUM_POS2': 0.3421400343512055, 'TW1': 0.6936432743119737, 'TW2': 0.32607541093461945, 'NUM_POS1_PER': 0.33025356962425156}\n",
      "\n",
      "NUM_POS2\n",
      "{'NUM_NEG2': 0.3084674758077302, 'NUM_NEU1': 0.3484913425862472, 'NUM_NEU2': 0.4764761594909855, 'NUM_POS1': 0.3421400343512055, 'TW1': 0.37837711931671797, 'TW2': 0.6737126465750586, 'NUM_POS2_PER': 0.30766894792873595}\n",
      "\n",
      "NUM_POS3\n",
      "{'NUM_NEG3': 0.304764618599842, 'NUM_NEU3': 0.5127505131212996, 'TW2': 0.3131379980045559, 'TW3': 0.7022283346780087}\n",
      "\n",
      "TW1\n",
      "{'NUM_NEG1': 0.5852331398670126, 'NUM_NEU1': 0.9603049811683173, 'NUM_NEU2': 0.5144154500339283, 'NUM_NEU3': 0.3062832897109994, 'NUM_POS1': 0.6936432743119737, 'NUM_POS2': 0.37837711931671797, 'TW2': 0.5379037359856472}\n",
      "\n",
      "TW2\n",
      "{'NUM_NEG2': 0.5793150972614369, 'NUM_NEU1': 0.5385755397393015, 'NUM_NEU2': 0.9613166884037355, 'NUM_NEU3': 0.4817824410448253, 'NUM_POS1': 0.32607541093461945, 'NUM_POS2': 0.6737126465750586, 'NUM_POS3': 0.3131379980045559, 'TW1': 0.5379037359856472, 'TW3': 0.48083802006146875}\n",
      "\n",
      "TW3\n",
      "{'NUM_NEG3': 0.6027214587212899, 'NUM_NEU1': 0.3055004638263273, 'NUM_NEU2': 0.4816590399585326, 'NUM_NEU3': 0.9580926218354631, 'NUM_POS3': 0.7022283346780087, 'TW2': 0.48083802006146875}\n",
      "\n",
      "CLOSE1\n",
      "{'CLOSE2': 0.9998146026034391, 'CLOSE3': 0.9996937537175759}\n",
      "\n",
      "CLOSE2\n",
      "{'CLOSE1': 0.9998146026034391, 'CLOSE3': 0.9998725791899129}\n",
      "\n",
      "CLOSE3\n",
      "{'CLOSE1': 0.9996937537175759, 'CLOSE2': 0.9998725791899129}\n",
      "\n",
      "NUM_NEG1_PER\n",
      "{'NUM_NEG1': 0.45800710807257766, 'NUM_NEG2_PER': 0.32228083035649757, 'NUM_NEU1_PER': -0.4891863070996612}\n",
      "\n",
      "NUM_NEG2_PER\n",
      "{'NUM_NEG2': 0.4202407650999104, 'NUM_NEG1_PER': 0.32228083035649757, 'NUM_NEG3_PER': 0.3162952174629858, 'NUM_NEU2_PER': -0.49325426805057004}\n",
      "\n",
      "NUM_NEG3_PER\n",
      "{'NUM_NEG3': 0.38717757134382996, 'NUM_NEG2_PER': 0.3162952174629858, 'NUM_NEU3_PER': -0.49075189593062823}\n",
      "\n",
      "NUM_NEU1_PER\n",
      "{'NUM_NEG1_PER': -0.4891863070996612, 'NUM_POS1_PER': -0.782745664452614}\n",
      "\n",
      "NUM_NEU2_PER\n",
      "{'NUM_NEG2_PER': -0.49325426805057004, 'NUM_POS2_PER': -0.7916559145088506}\n",
      "\n",
      "NUM_NEU3_PER\n",
      "{'NUM_NEG3_PER': -0.49075189593062823, 'NUM_POS3_PER': -0.79385185196034}\n",
      "\n",
      "NUM_POS1_PER\n",
      "{'NUM_POS1': 0.33025356962425156, 'NUM_NEU1_PER': -0.782745664452614}\n",
      "\n",
      "NUM_POS2_PER\n",
      "{'NUM_POS2': 0.30766894792873595, 'NUM_NEU2_PER': -0.7916559145088506}\n",
      "\n",
      "NUM_POS3_PER\n",
      "{'NUM_NEU3_PER': -0.79385185196034}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_corr = data_nopen_prev[best_feat2].corr()\n",
    "i_corr_extract(best_feat2, feature_corr, .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these correlations are quite obvious. Percentages are correlated with the number said that day and the number of any type of tweet on a given day is correlated with the other numbers. I might just drop all the numbers and reduce down to a pure percentage model, even though the correlation isn't super strong. The pure number of tweets on a given day is not correlated with the percentages though, so I think I'll leave those in as a sort of control. \n",
    "\n",
    "Unsurprisingly, the closing prices are very very very correlated with each other. I think I'll drop the numbers and then re-run the model to see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_prev = []\n",
    "for e in sentiments:\n",
    "    if e.endswith('1'):\n",
    "        sentiments_prev.append(e)\n",
    "    elif e.endswith('2'):\n",
    "        sentiments_prev.append(e)\n",
    "    elif e.endswith('3'):\n",
    "        sentiments_prev.append(e)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noprev_per = data_nopen_prev.drop(columns=sentiments_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(data_noprev_per)\n",
    "scaled_prevper = scaler.transform(data_noprev_per)\n",
    "\n",
    "X_trainP, X_testP, y_trainP, y_testP = train_test_split(scaled_prevper, target_class, test_size=.2, random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'n_neighbors': 11, 'p': 1, 'weights': 'distance'}\n",
      "Train F1: [1. 1. 1.]\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.65      0.61       810\n",
      "         1.0       0.42      0.42      0.42       630\n",
      "         2.0       0.39      0.31      0.35       592\n",
      "\n",
      "   micro avg       0.48      0.48      0.48      2032\n",
      "   macro avg       0.46      0.46      0.46      2032\n",
      "weighted avg       0.47      0.48      0.47      2032\n",
      "\n",
      "Train Accuracy: 1.0\tTest accuracy: 0.4812992125984252\n",
      "[[526 154 130]\n",
      " [207 267 156]\n",
      " [189 218 185]]\n"
     ]
    }
   ],
   "source": [
    "# K Nearest Neighbors without open raw numbers dropped\n",
    "knn = KNeighborsClassifier()\n",
    "gridsearch = GridSearchCV(knn, {\"n_neighbors\": [5, 7, 9, 11], \"weights\": ['uniform', 'distance'], \n",
    "                                'p': [1, 2, 3]}, scoring='f1_micro', cv=3)\n",
    "gridsearch.fit(X_trainP, y_trainP)\n",
    "print(\"Best Params: {}\".format(gridsearch.best_params_))\n",
    "y_pred_train = gridsearch.predict(X_trainP)\n",
    "print(\"Train F1: {}\".format(f1_score(y_trainP, y_pred_train, average=None)))\n",
    "print(\"Test Classification Report:\")\n",
    "y_pred_test = gridsearch.predict(X_testP)\n",
    "print(classification_report(y_testP, y_pred_test))\n",
    "print(\"Train Accuracy: {}\\tTest accuracy: {}\".format(accuracy_score(y_trainP, y_pred_train),\n",
    "                                                     accuracy_score(y_testP, y_pred_test)))\n",
    "print(confusion_matrix(y_testP, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-992e44d851fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 , \"class_weight\": [\"balanced\", \"balanced_subsample\"], 'random_state': [9]}, \n\u001b[1;32m      6\u001b[0m                           scoring='f1_micro', cv=3)\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgridsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trainP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Params: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgridsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgridsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 335\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    363\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Random Forest, raw numbers dropped\n",
    "rfc = RandomForestClassifier()\n",
    "gridsearch = GridSearchCV(rfc, {\"n_estimators\": [20, 50, 100, 200, 500], \"max_depth\": [1, 3, 5, 7, 9, 11], \n",
    "                                 \"min_samples_leaf\": [3 , 5 , 7 , 9], \"max_features\": [None, 'sqrt', 'log2']\n",
    "                                , \"class_weight\": [\"balanced\", \"balanced_subsample\"], 'random_state': [9]}, \n",
    "                          scoring='f1_micro', cv=3)\n",
    "gridsearch.fit(X_trainP, y_trainP)\n",
    "print(\"Best Params: {}\".format(gridsearch.best_params_))\n",
    "y_pred_train = gridsearch.predict(X_trainP)\n",
    "print(\"Train F1: {}\".format(f1_score(y_trainP, y_pred_train, average=None)))\n",
    "print(\"Test Classification Report:\")\n",
    "y_pred_test = gridsearch.predict(X_testP)\n",
    "print(classification_report(y_testP, y_pred_test))\n",
    "print(\"Train Accuracy: {}\\tTest accuracy: {}\".format(accuracy_score(y_trainP, y_pred_train),\n",
    "                                                     accuracy_score(y_testP, y_pred_test)))\n",
    "print(confusion_matrix(y_testP, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option for narrowing down the features list (and making this a bit less overfitting in the long run) is to use industries instead of dummies for each company. We used [this website](https://www.thestreet.com) to determine which companies were classified into which industries. Only two, United Health Group and du Pont had no comparable groups, so we just left those dummies as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial = ['AXP','GS','JPM','TRV']\n",
    "electronic_tech = ['BA', 'CSCO', 'INTC']\n",
    "producer_manu = ['CAT', 'GE', 'MMM','UTX']\n",
    "minerals = ['CVX','XOM']\n",
    "tech_services = ['IBM', 'MSFT','V']\n",
    "basic_materials = ['DD']\n",
    "consumer_services = ['DIS', 'MCD']\n",
    "retail = ['HD','WMT']\n",
    "health_tech = ['JNJ', 'MRK','PFE']\n",
    "health_services = ['UNH']\n",
    "non_durables = ['KO','NKE','PG']\n",
    "comms = ['T','VZ']\n",
    "\n",
    "industries = [financial, electronic_tech, producer_manu, minerals, consumer_services, tech_services, retail, \n",
    "              health_tech, non_durables, comms]\n",
    "\n",
    "def industrifier(df,list_, drop=True):\n",
    "    for i in range(len(df)):\n",
    "        for e in list_:\n",
    "            for x in e:\n",
    "                if df.loc[i,x] == 1:\n",
    "                    df.loc[i,str(e)] = 1\n",
    "                else:\n",
    "                    pass\n",
    "    if drop==True:\n",
    "        for e in list_:\n",
    "            df.drop(columns=e, inplace=True)\n",
    "        df.fillna(0,inplace=True)\n",
    "    else:\n",
    "        df.fillna(0,inplace=True)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
